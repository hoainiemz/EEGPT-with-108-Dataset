503 112 137
752
LitEEGPTCausal(
  (target_encoder): EEGTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(1, 128, kernel_size=(1, 200), stride=(1, 200))
    )
    (chan_embed): Embedding(62, 128)
    (blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=128, out_features=384, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (chan_conv): Conv1dWithConstraint(19, 19, kernel_size=(1,), stride=(1,))
  (meta_backbone): Sequential(
    (0): Linear(in_features=28, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.1, inplace=False)
  )
  (arranger): Rearrange('b c s d -> b (c s) d')
  (attention): ParamAttention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
    (drop): Dropout(p=0.1, inplace=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=1, bias=True)
    (1): Rearrange('b 1 -> (b 1)')
  )
  (backbone): Sequential(
    (0): Conv1dWithConstraint(19, 19, kernel_size=(1,), stride=(1,))
    (1): EEGTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(1, 128, kernel_size=(1, 200), stride=(1, 200))
      )
      (chan_embed): Embedding(62, 128)
      (blocks): ModuleList(
        (0-7): 8 x Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    )
  )
)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:02<00:00,  7.52it/s]
Epoch 1 : Training Loss: 0.71611, LR: 0.00030, Time elapsed 0.04 mins
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 24.55it/s]
val Evaluation: acc: 0.51786, pr_auc: 0.43377, roc_auc: 0.31537
[[52  4]
 [50  6]]
roc_auc increasing....saving weights !!
Val Evaluation: acc: 0.51786, pr_auc: 0.43377, roc_auc: 0.31537
 38%|███████████████████████████████████████▊                                                                  | 6/16 [00:00<00:01,  6.64it/s]
Traceback (most recent call last):
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/EEGPT/finetune_kfold_pearl.py", line 105, in <module>
    main()
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/EEGPT/finetune_kfold_pearl.py", line 80, in main
    acc_, pr_auc_, roc_auc_ = t.train_for_binaryclass()
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/EEGPT/finetune_trainer_kfold.py", line 94, in train_for_binaryclass
    self.optimizer.step()
  File "/mnt/disk1/aiotlab/envs/hoainiemeegpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/hoainiemeegpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/hoainiemeegpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/hoainiemeegpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 171, in step
    adamw(
  File "/mnt/disk1/aiotlab/envs/hoainiemeegpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 321, in adamw
    func(
  File "/mnt/disk1/aiotlab/envs/hoainiemeegpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 477, in _multi_tensor_adamw
    grouped_tensors = _group_tensors_by_device_and_dtype([
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/hoainiemeegpt/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/hoainiemeegpt/lib/python3.11/site-packages/torch/utils/_foreach_utils.py", line None, in _group_tensors_by_device_and_dtype
KeyboardInterrupt
