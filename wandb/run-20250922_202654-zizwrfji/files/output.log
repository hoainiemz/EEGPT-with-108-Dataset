503 112 137
752
LitEEGPTCausal(
  (target_encoder): EEGTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(1, 128, kernel_size=(1, 200), stride=(1, 200))
    )
    (chan_embed): Embedding(62, 128)
    (blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=128, out_features=384, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (chan_conv): Conv1dWithConstraint(19, 19, kernel_size=(1,), stride=(1,))
  (meta_backbone): Sequential(
    (0): Linear(in_features=28, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.1, inplace=False)
  )
  (arranger): Rearrange('b c s d -> b (c s) d')
  (attention): ParamAttention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
    (drop): Dropout(p=0.1, inplace=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=1, bias=True)
    (1): Rearrange('b 1 -> (b 1)')
  )
  (backbone): Sequential(
    (0): Conv1dWithConstraint(19, 19, kernel_size=(1,), stride=(1,))
    (1): EEGTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(1, 128, kernel_size=(1, 200), stride=(1, 200))
      )
      (chan_embed): Embedding(62, 128)
      (blocks): ModuleList(
        (0-7): 8 x Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    )
  )
)
 12%|█████████████▎                                                                                            | 2/16 [00:00<00:03,  4.14it/s]
Traceback (most recent call last):
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/EEGPT/finetune_kfold_pearl.py", line 105, in <module>
    main()
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/EEGPT/finetune_kfold_pearl.py", line 80, in main
    acc_, pr_auc_, roc_auc_ = t.train_for_binaryclass()
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/EEGPT/finetune_trainer_kfold.py", line 90, in train_for_binaryclass
    losses.append(loss.data.cpu().numpy())
                  ^^^^^^^^^^^^^^^
KeyboardInterrupt
