_wandb:
    value:
        cli_version: 0.21.4
        e:
            tl65d0390l9gp9l1oixaebp3w0h2temo:
                args:
                    - --project_name
                    - EEGPT-pearl
                    - --modality_mode
                    - multi
                    - --epochs
                    - "100"
                    - --batch_size
                    - "32"
                    - --lr
                    - "3e-4"
                    - --downstream_dataset
                    - PEARL
                    - --datasets_dir
                    - /mnt/disk1/aiotlab/namth/EEGFoundationModel/datasets/pearl_30s_oldnumpy/108
                    - --num_of_classes
                    - "2"
                    - --foundation_dir
                    - /mnt/disk1/aiotlab/namth/EEGFoundationModel/EEGPT/checkpoint/108.ckpt
                codePath: finetune_kfold_pearl.py
                codePathLocal: finetune_kfold_pearl.py
                cpu_count: 20
                cpu_count_logical: 40
                cudaVersion: "12.2"
                disk:
                    /:
                        total: "943412031488"
                        used: "764218912768"
                email: nam.th180805@gmail.com
                executable: /mnt/disk1/aiotlab/envs/hoainiemeegpt/bin/python
                git:
                    commit: a0e0a8fad729e2ecf4eedb3a81548a6e6d48a705
                    remote: https://github.com/BINE022/EEGPT.git
                gpu: NVIDIA A30
                gpu_count: 3
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 3584
                      memoryTotal: "25769803776"
                      name: NVIDIA A30
                      uuid: GPU-97533177-ae5d-752f-8fac-73c43a1a67cc
                    - architecture: Ampere
                      cudaCores: 3584
                      memoryTotal: "25769803776"
                      name: NVIDIA A30
                      uuid: GPU-b71aa699-b956-12eb-ded3-7590ec6c9937
                    - architecture: Ampere
                      cudaCores: 3584
                      memoryTotal: "25769803776"
                      name: NVIDIA A30
                      uuid: GPU-1ae02fa8-11a1-fd0d-1711-434d9c42d9ef
                host: gpus-Super-Server
                memory:
                    total: "134794272768"
                os: Linux-6.8.0-65-generic-x86_64-with-glibc2.35
                program: /mnt/disk1/aiotlab/namth/EEGFoundationModel/EEGPT/finetune_kfold_pearl.py
                python: CPython 3.11.13
                root: /mnt/disk1/aiotlab/namth/EEGFoundationModel/EEGPT
                startedAt: "2025-09-22T13:43:03.760791Z"
                writerId: tl65d0390l9gp9l1oixaebp3w0h2temo
        m: []
        python_version: 3.11.13
        t:
            "1":
                - 1
                - 5
                - 9
                - 11
                - 41
                - 49
                - 51
                - 53
                - 63
                - 71
                - 103
            "2":
                - 1
                - 5
                - 9
                - 11
                - 41
                - 49
                - 51
                - 53
                - 63
                - 71
                - 103
            "3":
                - 2
                - 13
                - 16
                - 61
            "4": 3.11.13
            "5": 0.21.4
            "6": 4.35.2
            "12": 0.21.4
            "13": linux-x86_64
batch_size:
    value: 32
clip_value:
    value: 1
cuda:
    value: 0
datasets_dir:
    value: /mnt/disk1/aiotlab/namth/EEGFoundationModel/datasets/pearl_30s_oldnumpy/108/fold_2
downstream_dataset:
    value: PEARL
dropout:
    value: 0.1
epochs:
    value: 100
foundation_dir:
    value: /mnt/disk1/aiotlab/namth/EEGFoundationModel/EEGPT/checkpoint/108.ckpt
frozen:
    value: false
img_size:
    value:
        - 19
        - 6000
label_smoothing:
    value: 0.1
lr:
    value: 0.0003
modality_mode:
    value: multi
model_dir:
    value: ./data/wjq/models_weights/Big/BigFaced
multi_lr:
    value: false
num_folds:
    value: -1
num_of_classes:
    value: 2
num_workers:
    value: 16
optimizer:
    value: AdamW
patch_size:
    value: 200
project_name:
    value: EEGPT-pearl
seed:
    value: 3407
use_pretrained_weights:
    value: true
weight_decay:
    value: 0.05
